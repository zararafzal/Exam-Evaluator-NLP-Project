# ðŸŽ“ Exam Evaluator AI

An automated exam grading system powered by NLP â€” built as a Bachelor's semester project.

## ðŸš€ Live Demo

Deploy instantly on [Streamlit Cloud](https://streamlit.io/cloud) â€” **free, no credit card required.**

---

## âœ¨ Features

| Feature | Description |
|---|---|
| ðŸ‘©â€ðŸ« **Teacher Portal** | Create exams, define model answers & keywords, view all submissions |
| ðŸ§‘â€ðŸŽ“ **Student Portal** | Join via exam code, submit answers, get instant feedback |
| ðŸ§  **NLP Grading** | TF-IDF cosine similarity + keyword matching + coherence scoring |
| ðŸ“Š **Analytics Dashboard** | Score distributions, class averages, per-question breakdown |
| âœï¸ **Score Override** | Teachers can manually adjust AI-assigned scores |
| â±ï¸ **Timed Exams** | Configurable countdown timer per exam |

---

## ðŸ§  How the AI Grading Works

Each student answer is graded using a weighted formula:

```
Final Score = MaxMarks Ã— [Semantic(0.50) + Keyword(0.30) + Coherence(0.20)]
```

| Signal | Weight | Method |
|---|---|---|
| Semantic Similarity | 50% | TF-IDF cosine similarity against model answer |
| Keyword Matching | 30% | Stemmed keyword presence check |
| Coherence | 20% | Answer length vs minimum word threshold |

---

## ðŸ› ï¸ Tech Stack

- **Frontend/Backend**: Streamlit (Python)
- **NLP Engine**: Custom TF-IDF + cosine similarity (pure Python + numpy)
- **Storage**: JSON files (local) / session state (Streamlit Cloud)
- **Auth**: SHA-256 password hashing

---

## ðŸ“¦ Deploy to Streamlit Cloud

### Step 1: Push to GitHub

```bash
git init
git add .
git commit -m "Initial commit: Exam Evaluator AI"
git branch -M main
git remote add origin https://github.com/YOUR_USERNAME/exam-evaluator.git
git push -u origin main
```

### Step 2: Deploy on Streamlit Cloud

1. Go to [share.streamlit.io](https://share.streamlit.io)
2. Sign in with GitHub
3. Click **"New app"**
4. Select your repo â†’ branch: `main` â†’ main file: `app.py`
5. Click **Deploy** â€” done! âœ…

---

## ðŸ’» Run Locally

```bash
# Clone the repo
git clone https://github.com/YOUR_USERNAME/exam-evaluator.git
cd exam-evaluator

# Install dependencies
pip install -r requirements.txt

# Run
streamlit run app.py
```

Open http://localhost:8501

---

## ðŸ“ Project Structure

```
exam-evaluator/
â”œâ”€â”€ app.py              # Main entry point & routing
â”œâ”€â”€ grader.py           # NLP grading engine
â”œâ”€â”€ database.py         # JSON-based persistence layer
â”œâ”€â”€ teacher_views.py    # Teacher UI (dashboard, create exam, results)
â”œâ”€â”€ student_views.py    # Student UI (take exam, view results)
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ config.toml     # Theme & server config
â”œâ”€â”€ data/               # Auto-created: stores users, exams, submissions
â””â”€â”€ README.md
```

---

## ðŸŽ¯ Usage Guide

### For Teachers
1. Register as a **Teacher**
2. Click **"Create New Exam"**
3. Fill in: title, subject, duration
4. Per question: write question text, model answer, marks, and optional keywords
5. Publish â†’ share the **6-character exam code** with students
6. View results anytime from your dashboard

### For Students
1. Register as a **Student**
2. Enter the **exam code** given by your teacher
3. Answer all questions within the time limit
4. Submit â†’ instantly see your score, grade, and per-question feedback

---

## ðŸ“Š Grading Score Table

| Cosine Similarity | Grade Contribution |
|---|---|
| 0.85 â€“ 1.00 | Excellent |
| 0.70 â€“ 0.84 | Good |
| 0.55 â€“ 0.69 | Partial |
| 0.40 â€“ 0.54 | Weak |
| 0.00 â€“ 0.39 | Off-topic |

---

## ðŸ“ CV Description

> **Exam Evaluator** â€” AI product using NLP to automate exam grading; defined end-to-end product flows, implemented TF-IDF semantic similarity evaluation logic, keyword matching pipeline, and usability requirements; deployed as a full-stack Streamlit web application with teacher and student portals.

---

## ðŸ‘¤ Author

Built as a Bachelor's Semester Project  
[Your Name] | [Your University] | [Year]
